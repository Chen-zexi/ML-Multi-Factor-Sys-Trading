{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized last_uid_grok from existing file: 20000\n",
      "Initialized last_uid_gemini from existing file: 209125\n",
      "Loading processed news data from ../../data/news/nasdaq_news_data_processed.csv...\n",
      "Loaded 209125 news items.\n"
     ]
    }
   ],
   "source": [
    "last_uid_grok = 0\n",
    "last_uid_gemini = 0\n",
    "sentiment_file_path_grok = '../../data/sentiment/news_sentiment_results.csv'\n",
    "sentiment_file_path_gemini = '../../data/sentiment/news_sentiment_results_gemini.csv'\n",
    "sentiment_file_path_gemini_hb = '../../data/sentiment/news_sentiment_results_gemini_hb.csv'\n",
    "processed_news_file_path = '../../data/news/nasdaq_news_data_processed.csv'\n",
    "\n",
    "sentiment_df_grok = pd.read_csv(sentiment_file_path_grok)\n",
    "sentiment_df_gemini = pd.read_csv(sentiment_file_path_gemini)\n",
    "sentiment_df_gemini_hb = pd.read_csv(sentiment_file_path_gemini_hb)\n",
    "\n",
    "last_uid_grok = sentiment_df_grok['UID'].max()\n",
    "last_uid_gemini = sentiment_df_gemini['UID'].max()\n",
    "print(f\"Initialized last_uid_grok from existing file: {last_uid_grok}\")\n",
    "print(f\"Initialized last_uid_gemini from existing file: {last_uid_gemini}\")\n",
    "\n",
    "print(f\"Loading processed news data from {processed_news_file_path}...\")\n",
    "news_df = pd.read_csv(processed_news_file_path, parse_dates=['Date'])\n",
    "print(f\"Loaded {len(news_df)} news items.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 209125 entries, 0 to 209124\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype         \n",
      "---  ------   --------------   -----         \n",
      " 0   UID      209125 non-null  int64         \n",
      " 1   Date     209125 non-null  datetime64[ns]\n",
      " 2   Ticker   209125 non-null  object        \n",
      " 3   Title    209125 non-null  object        \n",
      " 4   Summary  209125 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(3)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "news_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UID        0\n",
       "Date       0\n",
       "Ticker     0\n",
       "Title      0\n",
       "Summary    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 209125 entries, 0 to 209124\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype         \n",
      "---  ------   --------------   -----         \n",
      " 0   UID      209125 non-null  int64         \n",
      " 1   Date     209125 non-null  datetime64[ns]\n",
      " 2   Ticker   209125 non-null  object        \n",
      " 3   Title    209125 non-null  object        \n",
      " 4   Summary  209125 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(3)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "news_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total UIDs in sentiment_df: 142028\n",
      "Common UIDs between dataframes: 142028\n",
      "\n",
      "Alignment Results:\n",
      "Date mismatches: 0\n",
      "Ticker mismatches: 0\n",
      "Both Date and Ticker mismatches: 0\n",
      "Total mismatches: 0\n",
      "Alignment percentage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def check_alignment(sentiment_df, news_df):\n",
    "    \"\"\"\n",
    "    Check if sentiment_df and news_df are aligned by UID, Date, and Ticker.\n",
    "    Handles different date formats by comparing only year-month-day.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sentiment_df : DataFrame\n",
    "        The sentiment dataframe (smaller one) with Date as year-month-day\n",
    "    news_df : DataFrame\n",
    "        The news dataframe (larger one) with Date as datetime including time\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing alignment stats and mismatches\n",
    "    \"\"\"\n",
    "    # Get common UIDs\n",
    "    common_uids = set(sentiment_df['UID']).intersection(set(news_df['UID']))\n",
    "    \n",
    "    print(f\"Total UIDs in sentiment_df: {len(sentiment_df['UID'])}\")\n",
    "    print(f\"Common UIDs between dataframes: {len(common_uids)}\")\n",
    "    \n",
    "    # Check for UIDs in sentiment_df not found in news_df\n",
    "    missing_uids = set(sentiment_df['UID']) - set(news_df['UID'])\n",
    "    if missing_uids:\n",
    "        print(f\"Warning: {len(missing_uids)} UIDs in sentiment_df not found in news_df\")\n",
    "    \n",
    "    # Initialize results\n",
    "    results = {\n",
    "        'total_sentiment_uids': len(sentiment_df['UID']),\n",
    "        'common_uids': len(common_uids),\n",
    "        'missing_uids': list(missing_uids)[:10] if missing_uids else [],  # Show first 10 only\n",
    "        'date_mismatches': [],\n",
    "        'ticker_mismatches': [],\n",
    "        'both_mismatches': []\n",
    "    }\n",
    "    \n",
    "    # Check alignment for common UIDs\n",
    "    for uid in common_uids:\n",
    "        sentiment_row = sentiment_df[sentiment_df['UID'] == uid].iloc[0]\n",
    "        news_row = news_df[news_df['UID'] == uid].iloc[0]\n",
    "        \n",
    "        # Convert news_df datetime to date only for comparison\n",
    "        if isinstance(news_row['Date'], pd.Timestamp):\n",
    "            news_date = news_row['Date'].date()\n",
    "        else:\n",
    "            # Try to convert string to datetime and then get date part\n",
    "            try:\n",
    "                news_date = pd.to_datetime(news_row['Date']).date()\n",
    "            except:\n",
    "                news_date = news_row['Date']  # Keep as is if conversion fails\n",
    "        \n",
    "        # Convert sentiment_df date if needed\n",
    "        if not isinstance(sentiment_row['Date'], pd.Timestamp) and not hasattr(sentiment_row['Date'], 'date'):\n",
    "            try:\n",
    "                sentiment_date = pd.to_datetime(sentiment_row['Date']).date()\n",
    "            except:\n",
    "                sentiment_date = sentiment_row['Date']  # Keep as is if conversion fails\n",
    "        else:\n",
    "            sentiment_date = sentiment_row['Date'].date() if hasattr(sentiment_row['Date'], 'date') else sentiment_row['Date']\n",
    "        \n",
    "        # Compare date and ticker\n",
    "        date_match = str(sentiment_date) == str(news_date)\n",
    "        ticker_match = sentiment_row['Ticker'] == news_row['Ticker']\n",
    "        \n",
    "        if not date_match and not ticker_match:\n",
    "            results['both_mismatches'].append({\n",
    "                'UID': uid,\n",
    "                'sentiment_date': sentiment_date,\n",
    "                'news_date': news_date,\n",
    "                'sentiment_ticker': sentiment_row['Ticker'],\n",
    "                'news_ticker': news_row['Ticker']\n",
    "            })\n",
    "        elif not date_match:\n",
    "            results['date_mismatches'].append({\n",
    "                'UID': uid,\n",
    "                'sentiment_date': sentiment_date,\n",
    "                'news_date': news_date\n",
    "            })\n",
    "        elif not ticker_match:\n",
    "            results['ticker_mismatches'].append({\n",
    "                'UID': uid,\n",
    "                'sentiment_ticker': sentiment_row['Ticker'],\n",
    "                'news_ticker': news_row['Ticker']\n",
    "            })\n",
    "    \n",
    "    # Summary stats\n",
    "    results['date_mismatch_count'] = len(results['date_mismatches'])\n",
    "    results['ticker_mismatch_count'] = len(results['ticker_mismatches'])\n",
    "    results['both_mismatch_count'] = len(results['both_mismatches'])\n",
    "    results['total_mismatches'] = results['date_mismatch_count'] + results['ticker_mismatch_count'] + results['both_mismatch_count']\n",
    "    results['alignment_percentage'] = 100 * (len(common_uids) - results['total_mismatches']) / len(common_uids) if common_uids else 0\n",
    "    \n",
    "    print(f\"\\nAlignment Results:\")\n",
    "    print(f\"Date mismatches: {results['date_mismatch_count']}\")\n",
    "    print(f\"Ticker mismatches: {results['ticker_mismatch_count']}\")\n",
    "    print(f\"Both Date and Ticker mismatches: {results['both_mismatch_count']}\")\n",
    "    print(f\"Total mismatches: {results['total_mismatches']}\")\n",
    "    print(f\"Alignment percentage: {results['alignment_percentage']:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "results = check_alignment(sentiment_df_gemini, news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 63327 news items from 2023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[145799,\n",
       " 145800,\n",
       " 145801,\n",
       " 145802,\n",
       " 145803,\n",
       " 145804,\n",
       " 145805,\n",
       " 145806,\n",
       " 145807,\n",
       " 145808,\n",
       " 145809,\n",
       " 145810,\n",
       " 145811,\n",
       " 145812,\n",
       " 145813,\n",
       " 145814,\n",
       " 145815,\n",
       " 145816,\n",
       " 145817,\n",
       " 145818,\n",
       " 145819,\n",
       " 145820,\n",
       " 145821,\n",
       " 145822,\n",
       " 145823,\n",
       " 145824,\n",
       " 145825,\n",
       " 145826,\n",
       " 145827,\n",
       " 145828,\n",
       " 145829,\n",
       " 145830,\n",
       " 145831,\n",
       " 145832,\n",
       " 145833,\n",
       " 145834,\n",
       " 145835,\n",
       " 145836,\n",
       " 145837,\n",
       " 145838,\n",
       " 145839,\n",
       " 145840,\n",
       " 145841,\n",
       " 145842,\n",
       " 145843,\n",
       " 145844,\n",
       " 145845,\n",
       " 145846,\n",
       " 145847,\n",
       " 145848,\n",
       " 145849,\n",
       " 145850,\n",
       " 145851,\n",
       " 145852,\n",
       " 145853,\n",
       " 145854,\n",
       " 145855,\n",
       " 145856,\n",
       " 145857,\n",
       " 145858,\n",
       " 145859,\n",
       " 145860,\n",
       " 145861,\n",
       " 145862,\n",
       " 145863,\n",
       " 145864,\n",
       " 145865,\n",
       " 145866,\n",
       " 145867,\n",
       " 145868,\n",
       " 145869,\n",
       " 145870,\n",
       " 145871,\n",
       " 145872,\n",
       " 145873,\n",
       " 145874,\n",
       " 145875,\n",
       " 145876,\n",
       " 145877,\n",
       " 145878,\n",
       " 145879,\n",
       " 145880,\n",
       " 145881,\n",
       " 145882,\n",
       " 145883,\n",
       " 145884,\n",
       " 145885,\n",
       " 145886,\n",
       " 145887,\n",
       " 145888,\n",
       " 145889,\n",
       " 145890,\n",
       " 145891,\n",
       " 145892,\n",
       " 145893,\n",
       " 145894,\n",
       " 145895,\n",
       " 145896,\n",
       " 145897,\n",
       " 145898,\n",
       " 145899,\n",
       " 145900,\n",
       " 145901,\n",
       " 145902,\n",
       " 145903,\n",
       " 145904,\n",
       " 145905,\n",
       " 145906,\n",
       " 145907,\n",
       " 145908,\n",
       " 145909,\n",
       " 145910,\n",
       " 145911,\n",
       " 145912,\n",
       " 145913,\n",
       " 145914,\n",
       " 145915,\n",
       " 145916,\n",
       " 145917,\n",
       " 145918,\n",
       " 145919,\n",
       " 145920,\n",
       " 145921,\n",
       " 145922,\n",
       " 145923,\n",
       " 145924,\n",
       " 145925,\n",
       " 145926,\n",
       " 145927,\n",
       " 145928,\n",
       " 145929,\n",
       " 145930,\n",
       " 145931,\n",
       " 145932,\n",
       " 145933,\n",
       " 145934,\n",
       " 145935,\n",
       " 145936,\n",
       " 145937,\n",
       " 145938,\n",
       " 145939,\n",
       " 145940,\n",
       " 145941,\n",
       " 145942,\n",
       " 145943,\n",
       " 145944,\n",
       " 145945,\n",
       " 145946,\n",
       " 145947,\n",
       " 145948,\n",
       " 145949,\n",
       " 145950,\n",
       " 145951,\n",
       " 145952,\n",
       " 145953,\n",
       " 145954,\n",
       " 145955,\n",
       " 145956,\n",
       " 145957,\n",
       " 145958,\n",
       " 145959,\n",
       " 145960,\n",
       " 145961,\n",
       " 145962,\n",
       " 145963,\n",
       " 145964,\n",
       " 145965,\n",
       " 145966,\n",
       " 145967,\n",
       " 145968,\n",
       " 145969,\n",
       " 145970,\n",
       " 145971,\n",
       " 145972,\n",
       " 145973,\n",
       " 145974,\n",
       " 145975,\n",
       " 145976,\n",
       " 145977,\n",
       " 145978,\n",
       " 145979,\n",
       " 145980,\n",
       " 145981,\n",
       " 145982,\n",
       " 145983,\n",
       " 145984,\n",
       " 145985,\n",
       " 145986,\n",
       " 145987,\n",
       " 145988,\n",
       " 145989,\n",
       " 145990,\n",
       " 145991,\n",
       " 145992,\n",
       " 145993,\n",
       " 145994,\n",
       " 145995,\n",
       " 145996,\n",
       " 145997,\n",
       " 145998,\n",
       " 145999,\n",
       " 146000,\n",
       " 146001,\n",
       " 146002,\n",
       " 146003,\n",
       " 146004,\n",
       " 146005,\n",
       " 146006,\n",
       " 146007,\n",
       " 146008,\n",
       " 146009,\n",
       " 146010,\n",
       " 146011,\n",
       " 146012,\n",
       " 146013,\n",
       " 146014,\n",
       " 146015,\n",
       " 146016,\n",
       " 146017,\n",
       " 146018,\n",
       " 146019,\n",
       " 146020,\n",
       " 146021,\n",
       " 146022,\n",
       " 146023,\n",
       " 146024,\n",
       " 146025,\n",
       " 146026,\n",
       " 146027,\n",
       " 146028,\n",
       " 146029,\n",
       " 146030,\n",
       " 146031,\n",
       " 146032,\n",
       " 146033,\n",
       " 146034,\n",
       " 146035,\n",
       " 146036,\n",
       " 146037,\n",
       " 146038,\n",
       " 146039,\n",
       " 146040,\n",
       " 146041,\n",
       " 146042,\n",
       " 146043,\n",
       " 146044,\n",
       " 146045,\n",
       " 146046,\n",
       " 146047,\n",
       " 146048,\n",
       " 146049,\n",
       " 146050,\n",
       " 146051,\n",
       " 146052,\n",
       " 146053,\n",
       " 146054,\n",
       " 146055,\n",
       " 146056,\n",
       " 146057,\n",
       " 146058,\n",
       " 146059,\n",
       " 146060,\n",
       " 146061,\n",
       " 146062,\n",
       " 146063,\n",
       " 146064,\n",
       " 146065,\n",
       " 146066,\n",
       " 146067,\n",
       " 146068,\n",
       " 146069,\n",
       " 146070,\n",
       " 146071,\n",
       " 146072,\n",
       " 146073,\n",
       " 146074,\n",
       " 146075,\n",
       " 146076,\n",
       " 146077,\n",
       " 146078,\n",
       " 146079,\n",
       " 146080,\n",
       " 146081,\n",
       " 146082,\n",
       " 146083,\n",
       " 146084,\n",
       " 146085,\n",
       " 146086,\n",
       " 146087,\n",
       " 146088,\n",
       " 146089,\n",
       " 146090,\n",
       " 146091,\n",
       " 146092,\n",
       " 146093,\n",
       " 146094,\n",
       " 146095,\n",
       " 146096,\n",
       " 146097,\n",
       " 146098,\n",
       " 146099,\n",
       " 146100,\n",
       " 146101,\n",
       " 146102,\n",
       " 146103,\n",
       " 146104,\n",
       " 146105,\n",
       " 146106,\n",
       " 146107,\n",
       " 146108,\n",
       " 146109,\n",
       " 146110,\n",
       " 146111,\n",
       " 146112,\n",
       " 146113,\n",
       " 146114,\n",
       " 146115,\n",
       " 146116,\n",
       " 146117,\n",
       " 146118,\n",
       " 146119,\n",
       " 146120,\n",
       " 146121,\n",
       " 146122,\n",
       " 146123,\n",
       " 146124,\n",
       " 146125,\n",
       " 146126,\n",
       " 146127,\n",
       " 146128,\n",
       " 146129,\n",
       " 146130,\n",
       " 146131,\n",
       " 146132,\n",
       " 146133,\n",
       " 146134,\n",
       " 146135,\n",
       " 146136,\n",
       " 146137,\n",
       " 146138,\n",
       " 146139,\n",
       " 146140,\n",
       " 146141,\n",
       " 146142,\n",
       " 146143,\n",
       " 146144,\n",
       " 146145,\n",
       " 146146,\n",
       " 146147,\n",
       " 146148,\n",
       " 146149,\n",
       " 146150,\n",
       " 146151,\n",
       " 146152,\n",
       " 146153,\n",
       " 146154,\n",
       " 146155,\n",
       " 146156,\n",
       " 146157,\n",
       " 146158,\n",
       " 146159,\n",
       " 146160,\n",
       " 146161,\n",
       " 146162,\n",
       " 146163,\n",
       " 146164,\n",
       " 146165,\n",
       " 146166,\n",
       " 146167,\n",
       " 146168,\n",
       " 146169,\n",
       " 146170,\n",
       " 146171,\n",
       " 146172,\n",
       " 146173,\n",
       " 146174,\n",
       " 146175,\n",
       " 146176,\n",
       " 146177,\n",
       " 146178,\n",
       " 146179,\n",
       " 146180,\n",
       " 146181,\n",
       " 146182,\n",
       " 146183,\n",
       " 146184,\n",
       " 146185,\n",
       " 146186,\n",
       " 146187,\n",
       " 146188,\n",
       " 146189,\n",
       " 146190,\n",
       " 146191,\n",
       " 146192,\n",
       " 146193,\n",
       " 146194,\n",
       " 146195,\n",
       " 146196,\n",
       " 146197,\n",
       " 146198,\n",
       " 146199,\n",
       " 146200,\n",
       " 146201,\n",
       " 146202,\n",
       " 146203,\n",
       " 146204,\n",
       " 146205,\n",
       " 146206,\n",
       " 146207,\n",
       " 146208,\n",
       " 146209,\n",
       " 146210,\n",
       " 146211,\n",
       " 146212,\n",
       " 146213,\n",
       " 146214,\n",
       " 146215,\n",
       " 146216,\n",
       " 146217,\n",
       " 146218,\n",
       " 146219,\n",
       " 146220,\n",
       " 146221,\n",
       " 146222,\n",
       " 146223,\n",
       " 146224,\n",
       " 146225,\n",
       " 146226,\n",
       " 146227,\n",
       " 146228,\n",
       " 146229,\n",
       " 146230,\n",
       " 146231,\n",
       " 146232,\n",
       " 146233,\n",
       " 146234,\n",
       " 146235,\n",
       " 146236,\n",
       " 146237,\n",
       " 146238,\n",
       " 146239,\n",
       " 146240,\n",
       " 146241,\n",
       " 146242,\n",
       " 146243,\n",
       " 146244,\n",
       " 146245,\n",
       " 146246,\n",
       " 146247,\n",
       " 146248,\n",
       " 146249,\n",
       " 146250,\n",
       " 146251,\n",
       " 146252,\n",
       " 146253,\n",
       " 146254,\n",
       " 146255,\n",
       " 146256,\n",
       " 146257,\n",
       " 146258,\n",
       " 146259,\n",
       " 146260,\n",
       " 146261,\n",
       " 146262,\n",
       " 146263,\n",
       " 146264,\n",
       " 146265,\n",
       " 146266,\n",
       " 146267,\n",
       " 146268,\n",
       " 146269,\n",
       " 146270,\n",
       " 146271,\n",
       " 146272,\n",
       " 146273,\n",
       " 146274,\n",
       " 146275,\n",
       " 146276,\n",
       " 146277,\n",
       " 146278,\n",
       " 146279,\n",
       " 146280,\n",
       " 146281,\n",
       " 146282,\n",
       " 146283,\n",
       " 146284,\n",
       " 146285,\n",
       " 146286,\n",
       " 146287,\n",
       " 146288,\n",
       " 146289,\n",
       " 146290,\n",
       " 146291,\n",
       " 146292,\n",
       " 146293,\n",
       " 146294,\n",
       " 146295,\n",
       " 146296,\n",
       " 146297,\n",
       " 146298,\n",
       " 146299,\n",
       " 146300,\n",
       " 146301,\n",
       " 146302,\n",
       " 146303,\n",
       " 146304,\n",
       " 146305,\n",
       " 146306,\n",
       " 146307,\n",
       " 146308,\n",
       " 146309,\n",
       " 146310,\n",
       " 146311,\n",
       " 146312,\n",
       " 146313,\n",
       " 146314,\n",
       " 146315,\n",
       " 146316,\n",
       " 146317,\n",
       " 146318,\n",
       " 146319,\n",
       " 146320,\n",
       " 146321,\n",
       " 146322,\n",
       " 146323,\n",
       " 146324,\n",
       " 146325,\n",
       " 146326,\n",
       " 146327,\n",
       " 146328,\n",
       " 146329,\n",
       " 146330,\n",
       " 146331,\n",
       " 146332,\n",
       " 146333,\n",
       " 146334,\n",
       " 146335,\n",
       " 146336,\n",
       " 146337,\n",
       " 146338,\n",
       " 146339,\n",
       " 146340,\n",
       " 146341,\n",
       " 146342,\n",
       " 146343,\n",
       " 146344,\n",
       " 146345,\n",
       " 146346,\n",
       " 146347,\n",
       " 146348,\n",
       " 146349,\n",
       " 146350,\n",
       " 146351,\n",
       " 146352,\n",
       " 146353,\n",
       " 146354,\n",
       " 146355,\n",
       " 146356,\n",
       " 146357,\n",
       " 146358,\n",
       " 146359,\n",
       " 146360,\n",
       " 146361,\n",
       " 146362,\n",
       " 146363,\n",
       " 146364,\n",
       " 146365,\n",
       " 146366,\n",
       " 146367,\n",
       " 146368,\n",
       " 146369,\n",
       " 146370,\n",
       " 146371,\n",
       " 146372,\n",
       " 146373,\n",
       " 146374,\n",
       " 146375,\n",
       " 146376,\n",
       " 146377,\n",
       " 146378,\n",
       " 146379,\n",
       " 146380,\n",
       " 146381,\n",
       " 146382,\n",
       " 146383,\n",
       " 146384,\n",
       " 146385,\n",
       " 146386,\n",
       " 146387,\n",
       " 146388,\n",
       " 146389,\n",
       " 146390,\n",
       " 146391,\n",
       " 146392,\n",
       " 146393,\n",
       " 146394,\n",
       " 146395,\n",
       " 146396,\n",
       " 146397,\n",
       " 146398,\n",
       " 146399,\n",
       " 146400,\n",
       " 146401,\n",
       " 146402,\n",
       " 146403,\n",
       " 146404,\n",
       " 146405,\n",
       " 146406,\n",
       " 146407,\n",
       " 146408,\n",
       " 146409,\n",
       " 146410,\n",
       " 146411,\n",
       " 146412,\n",
       " 146413,\n",
       " 146414,\n",
       " 146415,\n",
       " 146416,\n",
       " 146417,\n",
       " 146418,\n",
       " 146419,\n",
       " 146420,\n",
       " 146421,\n",
       " 146422,\n",
       " 146423,\n",
       " 146424,\n",
       " 146425,\n",
       " 146426,\n",
       " 146427,\n",
       " 146428,\n",
       " 146429,\n",
       " 146430,\n",
       " 146431,\n",
       " 146432,\n",
       " 146433,\n",
       " 146434,\n",
       " 146435,\n",
       " 146436,\n",
       " 146437,\n",
       " 146438,\n",
       " 146439,\n",
       " 146440,\n",
       " 146441,\n",
       " 146442,\n",
       " 146443,\n",
       " 146444,\n",
       " 146445,\n",
       " 146446,\n",
       " 146447,\n",
       " 146448,\n",
       " 146449,\n",
       " 146450,\n",
       " 146451,\n",
       " 146452,\n",
       " 146453,\n",
       " 146454,\n",
       " 146455,\n",
       " 146456,\n",
       " 146457,\n",
       " 146458,\n",
       " 146459,\n",
       " 146460,\n",
       " 146461,\n",
       " 146462,\n",
       " 146463,\n",
       " 146464,\n",
       " 146465,\n",
       " 146466,\n",
       " 146467,\n",
       " 146468,\n",
       " 146469,\n",
       " 146470,\n",
       " 146471,\n",
       " 146472,\n",
       " 146473,\n",
       " 146474,\n",
       " 146475,\n",
       " 146476,\n",
       " 146477,\n",
       " 146478,\n",
       " 146479,\n",
       " 146480,\n",
       " 146481,\n",
       " 146482,\n",
       " 146483,\n",
       " 146484,\n",
       " 146485,\n",
       " 146486,\n",
       " 146487,\n",
       " 146488,\n",
       " 146489,\n",
       " 146490,\n",
       " 146491,\n",
       " 146492,\n",
       " 146493,\n",
       " 146494,\n",
       " 146495,\n",
       " 146496,\n",
       " 146497,\n",
       " 146498,\n",
       " 146499,\n",
       " 146500,\n",
       " 146501,\n",
       " 146502,\n",
       " 146503,\n",
       " 146504,\n",
       " 146505,\n",
       " 146506,\n",
       " 146507,\n",
       " 146508,\n",
       " 146509,\n",
       " 146510,\n",
       " 146511,\n",
       " 146512,\n",
       " 146513,\n",
       " 146514,\n",
       " 146515,\n",
       " 146516,\n",
       " 146517,\n",
       " 146518,\n",
       " 146519,\n",
       " 146520,\n",
       " 146521,\n",
       " 146522,\n",
       " 146523,\n",
       " 146524,\n",
       " 146525,\n",
       " 146526,\n",
       " 146527,\n",
       " 146528,\n",
       " 146529,\n",
       " 146530,\n",
       " 146531,\n",
       " 146532,\n",
       " 146533,\n",
       " 146534,\n",
       " 146535,\n",
       " 146536,\n",
       " 146537,\n",
       " 146538,\n",
       " 146539,\n",
       " 146540,\n",
       " 146541,\n",
       " 146542,\n",
       " 146543,\n",
       " 146544,\n",
       " 146545,\n",
       " 146546,\n",
       " 146547,\n",
       " 146548,\n",
       " 146549,\n",
       " 146550,\n",
       " 146551,\n",
       " 146552,\n",
       " 146553,\n",
       " 146554,\n",
       " 146555,\n",
       " 146556,\n",
       " 146557,\n",
       " 146558,\n",
       " 146559,\n",
       " 146560,\n",
       " 146561,\n",
       " 146562,\n",
       " 146563,\n",
       " 146564,\n",
       " 146565,\n",
       " 146566,\n",
       " 146567,\n",
       " 146568,\n",
       " 146569,\n",
       " 146570,\n",
       " 146571,\n",
       " 146572,\n",
       " 146573,\n",
       " 146574,\n",
       " 146575,\n",
       " 146576,\n",
       " 146577,\n",
       " 146578,\n",
       " 146579,\n",
       " 146580,\n",
       " 146581,\n",
       " 146582,\n",
       " 146583,\n",
       " 146584,\n",
       " 146585,\n",
       " 146586,\n",
       " 146587,\n",
       " 146588,\n",
       " 146589,\n",
       " 146590,\n",
       " 146591,\n",
       " 146592,\n",
       " 146593,\n",
       " 146594,\n",
       " 146595,\n",
       " 146596,\n",
       " 146597,\n",
       " 146598,\n",
       " 146599,\n",
       " 146600,\n",
       " 146601,\n",
       " 146602,\n",
       " 146603,\n",
       " 146604,\n",
       " 146605,\n",
       " 146606,\n",
       " 146607,\n",
       " 146608,\n",
       " 146609,\n",
       " 146610,\n",
       " 146611,\n",
       " 146612,\n",
       " 146613,\n",
       " 146614,\n",
       " 146615,\n",
       " 146616,\n",
       " 146617,\n",
       " 146618,\n",
       " 146619,\n",
       " 146620,\n",
       " 146621,\n",
       " 146622,\n",
       " 146623,\n",
       " 146624,\n",
       " 146625,\n",
       " 146626,\n",
       " 146627,\n",
       " 146628,\n",
       " 146629,\n",
       " 146630,\n",
       " 146631,\n",
       " 146632,\n",
       " 146633,\n",
       " 146634,\n",
       " 146635,\n",
       " 146636,\n",
       " 146637,\n",
       " 146638,\n",
       " 146639,\n",
       " 146640,\n",
       " 146641,\n",
       " 146642,\n",
       " 146643,\n",
       " 146644,\n",
       " 146645,\n",
       " 146646,\n",
       " 146647,\n",
       " 146648,\n",
       " 146649,\n",
       " 146650,\n",
       " 146651,\n",
       " 146652,\n",
       " 146653,\n",
       " 146654,\n",
       " 146655,\n",
       " 146656,\n",
       " 146657,\n",
       " 146658,\n",
       " 146659,\n",
       " 146660,\n",
       " 146661,\n",
       " 146662,\n",
       " 146663,\n",
       " 146664,\n",
       " 146665,\n",
       " 146666,\n",
       " 146667,\n",
       " 146668,\n",
       " 146669,\n",
       " 146670,\n",
       " 146671,\n",
       " 146672,\n",
       " 146673,\n",
       " 146674,\n",
       " 146675,\n",
       " 146676,\n",
       " 146677,\n",
       " 146678,\n",
       " 146679,\n",
       " 146680,\n",
       " 146681,\n",
       " 146682,\n",
       " 146683,\n",
       " 146684,\n",
       " 146685,\n",
       " 146686,\n",
       " 146687,\n",
       " 146688,\n",
       " 146689,\n",
       " 146690,\n",
       " 146691,\n",
       " 146692,\n",
       " 146693,\n",
       " 146694,\n",
       " 146695,\n",
       " 146696,\n",
       " 146697,\n",
       " 146698,\n",
       " 146699,\n",
       " 146700,\n",
       " 146701,\n",
       " 146702,\n",
       " 146703,\n",
       " 146704,\n",
       " 146705,\n",
       " 146706,\n",
       " 146707,\n",
       " 146708,\n",
       " 146709,\n",
       " 146710,\n",
       " 146711,\n",
       " 146712,\n",
       " 146713,\n",
       " 146714,\n",
       " 146715,\n",
       " 146716,\n",
       " 146717,\n",
       " 146718,\n",
       " 146719,\n",
       " 146720,\n",
       " 146721,\n",
       " 146722,\n",
       " 146723,\n",
       " 146724,\n",
       " 146725,\n",
       " 146726,\n",
       " 146727,\n",
       " 146728,\n",
       " 146729,\n",
       " 146730,\n",
       " 146731,\n",
       " 146732,\n",
       " 146733,\n",
       " 146734,\n",
       " 146735,\n",
       " 146736,\n",
       " 146737,\n",
       " 146738,\n",
       " 146739,\n",
       " 146740,\n",
       " 146741,\n",
       " 146742,\n",
       " 146743,\n",
       " 146744,\n",
       " 146745,\n",
       " 146746,\n",
       " 146747,\n",
       " 146748,\n",
       " 146749,\n",
       " 146750,\n",
       " 146751,\n",
       " 146752,\n",
       " 146753,\n",
       " 146754,\n",
       " 146755,\n",
       " 146756,\n",
       " 146757,\n",
       " 146758,\n",
       " 146759,\n",
       " 146760,\n",
       " 146761,\n",
       " 146762,\n",
       " 146763,\n",
       " 146764,\n",
       " 146765,\n",
       " 146766,\n",
       " 146767,\n",
       " 146768,\n",
       " 146769,\n",
       " 146770,\n",
       " 146771,\n",
       " 146772,\n",
       " 146773,\n",
       " 146774,\n",
       " 146775,\n",
       " 146776,\n",
       " 146777,\n",
       " 146778,\n",
       " 146779,\n",
       " 146780,\n",
       " 146781,\n",
       " 146782,\n",
       " 146783,\n",
       " 146784,\n",
       " 146785,\n",
       " 146786,\n",
       " 146787,\n",
       " 146788,\n",
       " 146789,\n",
       " 146790,\n",
       " 146791,\n",
       " 146792,\n",
       " 146793,\n",
       " 146794,\n",
       " 146795,\n",
       " 146796,\n",
       " 146797,\n",
       " 146798,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_news_uids_by_years(df, years=[2023]):\n",
    "    \"\"\"\n",
    "    Find UIDs for news data from specified years.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing news data with 'Date' and 'UID' columns\n",
    "    years : list, default [2022, 2023]\n",
    "        List of years to filter data by\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with years as keys and lists of UIDs as values\n",
    "    \"\"\"\n",
    "    # Make sure Date column is in datetime format\n",
    "    if not pd.api.types.is_datetime64_dtype(df['Date']):\n",
    "        df = df.copy()\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Extract year from Date\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {}\n",
    "    \n",
    "    # Get UIDs for each year\n",
    "    for year in years:\n",
    "        year_uids = df[df['Year'] == year]['UID'].tolist()\n",
    "        result[year] = year_uids\n",
    "        print(f\"Found {len(year_uids)} news items from {year}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "uids_by_year = get_news_uids_by_years(news_df)\n",
    "    \n",
    "uids_by_year[2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_daily_unique_tickers_per_year(df, date_column = 'Date', ticker_column = 'Ticker'):\n",
    "    \"\"\"\n",
    "    Calculates the average number of unique tickers with news per day, for each year.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        date_column (str): The name of the column containing the dates.\n",
    "        ticker_column (str): The name of the column containing the tickers.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are the years (int) and values are the\n",
    "              average number of unique tickers appearing per day in that year.\n",
    "              Returns None if a critical error occurs, or an empty dict if no valid data.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Convert the date column to datetime objects, coercing errors\n",
    "    df_copy[date_column] = pd.to_datetime(df_copy[date_column], errors='coerce')\n",
    "\n",
    "    # Drop rows where date conversion failed or ticker is missing\n",
    "    initial_rows = len(df_copy)\n",
    "    df_copy.dropna(subset=[date_column, ticker_column], inplace=True)\n",
    "    dropped_rows = initial_rows - len(df_copy)\n",
    "    if dropped_rows > 0:\n",
    "        print(f\"Warning: Dropped {dropped_rows} rows with invalid dates or missing tickers.\")\n",
    "\n",
    "    if df_copy.empty:\n",
    "        print(\"DataFrame is empty after handling invalid dates/tickers.\")\n",
    "        return {}\n",
    "\n",
    "    # Extract year and date\n",
    "    df_copy['Year'] = df_copy[date_column].dt.year\n",
    "    df_copy['DateOnly'] = df_copy[date_column].dt.date\n",
    "\n",
    "    # --- Calculate Number of Unique Tickers Per Day ---\n",
    "    # Group by Year and DateOnly, then count the number of unique tickers within each day's group.\n",
    "    unique_tickers_per_day = df_copy.groupby(['Year', 'DateOnly'])[ticker_column].nunique()\n",
    "\n",
    "    if unique_tickers_per_day.empty:\n",
    "        print(\"No valid data found to group by Year and Date.\")\n",
    "        return {}\n",
    "\n",
    "    # --- Calculate the Average of Daily Unique Ticker Counts per Year ---\n",
    "    # Group the daily unique counts by Year and calculate the mean.\n",
    "    avg_unique_tickers_per_day_by_year = unique_tickers_per_day.groupby('Year').mean()\n",
    "\n",
    "    yearly_results = {}\n",
    "    print(\"\\n--- Average Unique Tickers Per Day By Year ---\")\n",
    "    for year, avg_count in avg_unique_tickers_per_day_by_year.items():\n",
    "        yearly_results[year] = avg_count\n",
    "        print(f\"  Year {year}: {avg_count:.2f} unique tickers per day (on average)\")\n",
    "\n",
    "    return yearly_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_daily_unique_tickers_per_year(sentiment_df, 'Date', 'Ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_direction(sentiment):\n",
    "    \"\"\"Determine the direction of a sentiment value.\"\"\"\n",
    "    sentiment_mapping = {\n",
    "        \"Strongly Bearish\": -3, \"Bearish\": -2, \"Slightly Bearish\": -1,\n",
    "        \"Neutral\": 0,\n",
    "        \"Slightly Bullish\": 1, \"Bullish\": 2, \"Strongly Bullish\": 3\n",
    "    }\n",
    "    \n",
    "    if isinstance(sentiment, int) and 1 <= sentiment <= 7:\n",
    "        # Convert 1-7 scale to direction\n",
    "        if sentiment < 4:\n",
    "            return \"Bearish\"\n",
    "        elif sentiment == 4:\n",
    "            return \"Neutral\"\n",
    "        else:\n",
    "            return \"Bullish\"\n",
    "    elif isinstance(sentiment, str) and sentiment in sentiment_mapping:\n",
    "        # String sentiment\n",
    "        if sentiment_mapping[sentiment] < 0:\n",
    "            return \"Bearish\"\n",
    "        elif sentiment_mapping[sentiment] == 0:\n",
    "            return \"Neutral\"\n",
    "        else:\n",
    "            return \"Bullish\"\n",
    "    \n",
    "    # Return None if sentiment format is unexpected\n",
    "    return None\n",
    "\n",
    "def compare_sentiment_in_range(df1: pd.DataFrame, df2: pd.DataFrame, \n",
    "                               start_uid: int, end_uid: int,\n",
    "                               uid_col: str = 'UID',\n",
    "                               date_col: str = 'Date',\n",
    "                               ticker_col: str = 'Ticker',\n",
    "                               sentiment_col: str = 'Sentiment',\n",
    "                               df1_name: str = 'Model1', \n",
    "                               df2_name: str = 'Model2') -> dict:\n",
    "    \"\"\"\n",
    "    Compares two sentiment dataframes within a specified UID range,\n",
    "    checking if the Date and Ticker values match for each common UID.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df1 : DataFrame\n",
    "        First sentiment dataframe\n",
    "    df2 : DataFrame\n",
    "        Second sentiment dataframe\n",
    "    start_uid : int\n",
    "        Start of UID range to evaluate\n",
    "    end_uid : int\n",
    "        End of UID range to evaluate\n",
    "    uid_col : str\n",
    "        Name of the UID column\n",
    "    date_col : str\n",
    "        Name of the Date column\n",
    "    ticker_col : str\n",
    "        Name of the Ticker column\n",
    "    sentiment_col : str\n",
    "        Name of the Sentiment column\n",
    "    df1_name : str\n",
    "        Name identifier for the first dataframe\n",
    "    df2_name : str\n",
    "        Name identifier for the second dataframe\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing alignment stats and mismatches\n",
    "    \"\"\"\n",
    "    # Filter dataframes by UID range\n",
    "    df1_filtered = df1[(df1[uid_col] >= start_uid) & (df1[uid_col] <= end_uid)].copy()\n",
    "    df2_filtered = df2[(df2[uid_col] >= start_uid) & (df2[uid_col] <= end_uid)].copy()\n",
    "    \n",
    "    print(f\"UID Range Analysis: {start_uid} to {end_uid}\")\n",
    "    print(f\"Records in {df1_name}: {len(df1_filtered)}\")\n",
    "    print(f\"Records in {df2_name}: {len(df2_filtered)}\")\n",
    "    \n",
    "    # Find common UIDs in the range\n",
    "    common_uids = set(df1_filtered[uid_col]).intersection(set(df2_filtered[uid_col]))\n",
    "    print(f\"Common UIDs between dataframes in range: {len(common_uids)}\")\n",
    "    \n",
    "    # Check for UIDs in range present in one df but not the other\n",
    "    only_in_df1 = set(df1_filtered[uid_col]) - set(df2_filtered[uid_col])\n",
    "    only_in_df2 = set(df2_filtered[uid_col]) - set(df1_filtered[uid_col])\n",
    "    \n",
    "    if only_in_df1:\n",
    "        print(f\"UIDs present only in {df1_name}: {len(only_in_df1)}\")\n",
    "    if only_in_df2:\n",
    "        print(f\"UIDs present only in {df2_name}: {len(only_in_df2)}\")\n",
    "    \n",
    "    # Initialize results\n",
    "    results = {\n",
    "        'uid_range': {'start': start_uid, 'end': end_uid},\n",
    "        'total_records': {df1_name: len(df1_filtered), df2_name: len(df2_filtered)},\n",
    "        'common_uids': len(common_uids),\n",
    "        'only_in_first_df': list(only_in_df1)[:10] if only_in_df1 else [],  # First 10 examples\n",
    "        'only_in_second_df': list(only_in_df2)[:10] if only_in_df2 else [],  # First 10 examples\n",
    "        'date_mismatches': [],\n",
    "        'ticker_mismatches': [],\n",
    "        'both_mismatches': [],\n",
    "        'sentiment_comparison': {}\n",
    "    }\n",
    "    \n",
    "    # Check alignment for common UIDs\n",
    "    date_mismatch_count = 0\n",
    "    ticker_mismatch_count = 0\n",
    "    both_mismatch_count = 0\n",
    "    exact_match_count = 0\n",
    "    \n",
    "    # Fine-grained sentiment agreement categories\n",
    "    exact_agreement = 0\n",
    "    moderate_agreement = 0\n",
    "    slight_disagreement = 0\n",
    "    disagreement = 0\n",
    "    unknown_comparison = 0\n",
    "    sentiment_comparison_examples = {\n",
    "        'exact_agreement': [],\n",
    "        'moderate_agreement': [],\n",
    "        'slight_disagreement': [],\n",
    "        'disagreement': []\n",
    "    }\n",
    "    \n",
    "    for uid in common_uids:\n",
    "        row1 = df1_filtered[df1_filtered[uid_col] == uid].iloc[0]\n",
    "        row2 = df2_filtered[df2_filtered[uid_col] == uid].iloc[0]\n",
    "        \n",
    "        # Convert dates to compatible format for comparison\n",
    "        if isinstance(row1[date_col], pd.Timestamp) or isinstance(row2[date_col], pd.Timestamp):\n",
    "            date1 = pd.to_datetime(row1[date_col]).date() if hasattr(pd.to_datetime(row1[date_col]), 'date') else pd.to_datetime(row1[date_col])\n",
    "            date2 = pd.to_datetime(row2[date_col]).date() if hasattr(pd.to_datetime(row2[date_col]), 'date') else pd.to_datetime(row2[date_col])\n",
    "            date_match = str(date1) == str(date2)\n",
    "        else:\n",
    "            date_match = str(row1[date_col]) == str(row2[date_col])\n",
    "            \n",
    "        ticker_match = row1[ticker_col] == row2[ticker_col]\n",
    "        \n",
    "        # Check sentiment agreement if metadata matches\n",
    "        if date_match and ticker_match:\n",
    "            exact_match_count += 1\n",
    "            \n",
    "            # Compare sentiment if both have it\n",
    "            if sentiment_col in row1.index and sentiment_col in row2.index:\n",
    "                sentiment1 = row1[sentiment_col]\n",
    "                sentiment2 = row2[sentiment_col]\n",
    "                \n",
    "                # Exact match\n",
    "                if sentiment1 == sentiment2:\n",
    "                    exact_agreement += 1\n",
    "                    sentiment_comparison_examples['exact_agreement'].append({\n",
    "                        'UID': uid,\n",
    "                        f'{df1_name}_sentiment': sentiment1,\n",
    "                        f'{df2_name}_sentiment': sentiment2,\n",
    "                        'category': 'Exact agreement'\n",
    "                    })\n",
    "                else:\n",
    "                    # Determine directions\n",
    "                    direction1 = get_sentiment_direction(sentiment1)\n",
    "                    direction2 = get_sentiment_direction(sentiment2)\n",
    "                    \n",
    "                    if direction1 is None or direction2 is None:\n",
    "                        unknown_comparison += 1\n",
    "                    # Disagreement: opposite directions\n",
    "                    elif (direction1 == \"Bearish\" and direction2 == \"Bullish\") or (direction1 == \"Bullish\" and direction2 == \"Bearish\"):\n",
    "                        disagreement += 1\n",
    "                        sentiment_comparison_examples['disagreement'].append({\n",
    "                            'UID': uid,\n",
    "                            f'{df1_name}_sentiment': sentiment1,\n",
    "                            f'{df2_name}_sentiment': sentiment2,\n",
    "                            'category': 'Disagreement (opposite directions)'\n",
    "                        })\n",
    "                    # Slight disagreement: neutral vs bullish/bearish\n",
    "                    elif (direction1 == \"Neutral\" and direction2 in [\"Bullish\", \"Bearish\"]) or (direction2 == \"Neutral\" and direction1 in [\"Bullish\", \"Bearish\"]):\n",
    "                        slight_disagreement += 1\n",
    "                        sentiment_comparison_examples['slight_disagreement'].append({\n",
    "                            'UID': uid,\n",
    "                            f'{df1_name}_sentiment': sentiment1,\n",
    "                            f'{df2_name}_sentiment': sentiment2,\n",
    "                            'category': 'Slight disagreement (neutral vs directional)'\n",
    "                        })\n",
    "                    # Moderate agreement: same direction but different intensity\n",
    "                    elif direction1 == direction2:\n",
    "                        moderate_agreement += 1\n",
    "                        sentiment_comparison_examples['moderate_agreement'].append({\n",
    "                            'UID': uid,\n",
    "                            f'{df1_name}_sentiment': sentiment1,\n",
    "                            f'{df2_name}_sentiment': sentiment2,\n",
    "                            'category': 'Moderate agreement (same direction)'\n",
    "                        })\n",
    "        \n",
    "        # Track mismatches\n",
    "        if not date_match and not ticker_match:\n",
    "            both_mismatch_count += 1\n",
    "            results['both_mismatches'].append({\n",
    "                'UID': uid,\n",
    "                f'{df1_name}_date': row1[date_col],\n",
    "                f'{df2_name}_date': row2[date_col],\n",
    "                f'{df1_name}_ticker': row1[ticker_col],\n",
    "                f'{df2_name}_ticker': row2[ticker_col]\n",
    "            })\n",
    "        elif not date_match:\n",
    "            date_mismatch_count += 1\n",
    "            results['date_mismatches'].append({\n",
    "                'UID': uid,\n",
    "                f'{df1_name}_date': row1[date_col],\n",
    "                f'{df2_name}_date': row2[date_col]\n",
    "            })\n",
    "        elif not ticker_match:\n",
    "            ticker_mismatch_count += 1\n",
    "            results['ticker_mismatches'].append({\n",
    "                'UID': uid,\n",
    "                f'{df1_name}_ticker': row1[ticker_col],\n",
    "                f'{df2_name}_ticker': row2[ticker_col]\n",
    "            })\n",
    "    \n",
    "    # Summary stats\n",
    "    total_mismatches = date_mismatch_count + ticker_mismatch_count + both_mismatch_count\n",
    "    alignment_percentage = 100 * exact_match_count / len(common_uids) if common_uids else 0\n",
    "    \n",
    "    results['summary'] = {\n",
    "        'exact_matches': exact_match_count,\n",
    "        'date_mismatches': date_mismatch_count,\n",
    "        'ticker_mismatches': ticker_mismatch_count,\n",
    "        'both_mismatches': both_mismatch_count,\n",
    "        'total_mismatches': total_mismatches,\n",
    "        'alignment_percentage': alignment_percentage\n",
    "    }\n",
    "    \n",
    "    # Calculate total sentiment comparisons\n",
    "    total_sentiment_comparisons = exact_agreement + moderate_agreement + slight_disagreement + disagreement + unknown_comparison\n",
    "    \n",
    "    if total_sentiment_comparisons > 0:\n",
    "        results['sentiment_comparison'] = {\n",
    "            'exact_agreement': exact_agreement,\n",
    "            'moderate_agreement': moderate_agreement,\n",
    "            'slight_disagreement': slight_disagreement,\n",
    "            'disagreement': disagreement,\n",
    "            'unknown': unknown_comparison,\n",
    "            'total': total_sentiment_comparisons,\n",
    "            'examples': sentiment_comparison_examples\n",
    "        }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nAlignment Results:\")\n",
    "    print(f\"Exact matches: {exact_match_count} ({alignment_percentage:.2f}%)\")\n",
    "    print(f\"Date mismatches: {date_mismatch_count}\")\n",
    "    print(f\"Ticker mismatches: {ticker_mismatch_count}\")\n",
    "    print(f\"Both Date and Ticker mismatches: {both_mismatch_count}\")\n",
    "    print(f\"Total mismatches: {total_mismatches}\")\n",
    "    \n",
    "    if total_sentiment_comparisons > 0:\n",
    "        print(f\"\\nSentiment Comparison (for matched records):\")\n",
    "        print(f\"Exact agreement: {exact_agreement} ({100*exact_agreement/total_sentiment_comparisons:.2f}%)\")\n",
    "        print(f\"Moderate agreement (same direction): {moderate_agreement} ({100*moderate_agreement/total_sentiment_comparisons:.2f}%)\")\n",
    "        print(f\"Slight disagreement (neutral vs directional): {slight_disagreement} ({100*slight_disagreement/total_sentiment_comparisons:.2f}%)\")\n",
    "        print(f\"Disagreement (opposite directions): {disagreement} ({100*disagreement/total_sentiment_comparisons:.2f}%)\")\n",
    "        if unknown_comparison > 0:\n",
    "            print(f\"Unknown comparisons: {unknown_comparison}\")\n",
    "    \n",
    "    # Show sample mismatches if any\n",
    "    if results['date_mismatches']:\n",
    "        print(\"\\nSample Date mismatches:\")\n",
    "        for i, mismatch in enumerate(results['date_mismatches'][:3]):\n",
    "            print(f\"  {i+1}. UID: {mismatch['UID']}, {df1_name} Date: {mismatch[f'{df1_name}_date']}, {df2_name} Date: {mismatch[f'{df2_name}_date']}\")\n",
    "    \n",
    "    if results['ticker_mismatches']:\n",
    "        print(\"\\nSample Ticker mismatches:\")\n",
    "        for i, mismatch in enumerate(results['ticker_mismatches'][:3]):\n",
    "            print(f\"  {i+1}. UID: {mismatch['UID']}, {df1_name} Ticker: {mismatch[f'{df1_name}_ticker']}, {df2_name} Ticker: {mismatch[f'{df2_name}_ticker']}\")\n",
    "    \n",
    "    # Show sample sentiment comparisons\n",
    "    for category, examples in sentiment_comparison_examples.items():\n",
    "        if examples:\n",
    "            print(f\"\\nSample {category.replace('_', ' ').title()}:\")\n",
    "            for i, example in enumerate(examples[:2]):  # Show only 2 examples per category\n",
    "                print(f\"  {i+1}. UID: {example['UID']}, {df1_name}: {example[f'{df1_name}_sentiment']}, {df2_name}: {example[f'{df2_name}_sentiment']}\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID Range Analysis: 1 to 1000\n",
      "Records in Sentiment: 1000\n",
      "Records in News: 1000\n",
      "Common UIDs between dataframes in range: 1000\n",
      "\n",
      "Alignment Results:\n",
      "Exact matches: 1000 (100.00%)\n",
      "Date mismatches: 0\n",
      "Ticker mismatches: 0\n",
      "Both Date and Ticker mismatches: 0\n",
      "Total mismatches: 0\n",
      "\n",
      "Sentiment Comparison (for matched records):\n",
      "Exact agreement: 587 (58.70%)\n",
      "Moderate agreement (same direction): 166 (16.60%)\n",
      "Slight disagreement (neutral vs directional): 171 (17.10%)\n",
      "Disagreement (opposite directions): 16 (1.60%)\n",
      "Unknown comparisons: 60\n",
      "\n",
      "Sample Exact Agreement:\n",
      "  1. UID: 1, Sentiment: Bullish, News: Bullish\n",
      "  2. UID: 3, Sentiment: Neutral, News: Neutral\n",
      "\n",
      "Sample Moderate Agreement:\n",
      "  1. UID: 6, Sentiment: Slightly Bullish, News: Bullish\n",
      "  2. UID: 9, Sentiment: Bullish, News: Slightly Bullish\n",
      "\n",
      "Sample Slight Disagreement:\n",
      "  1. UID: 2, Sentiment: Bullish, News: Neutral\n",
      "  2. UID: 8, Sentiment: Neutral, News: Slightly Bullish\n",
      "\n",
      "Sample Disagreement:\n",
      "  1. UID: 29, Sentiment: Slightly Bearish, News: Slightly Bullish\n",
      "  2. UID: 142, Sentiment: Slightly Bearish, News: Slightly Bullish\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# Example usage:\n",
    "results = compare_sentiment_in_range(sentiment_df_gemini_hb, sentiment_df_gemini, \n",
    "                                    start_uid=1, end_uid=1000, \n",
    "                                    df1_name='Sentiment', df2_name='News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>1021</td>\n",
       "      <td>2015-02-09</td>\n",
       "      <td>NTES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>1022</td>\n",
       "      <td>2015-02-09</td>\n",
       "      <td>VRSN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>1023</td>\n",
       "      <td>2015-02-09</td>\n",
       "      <td>AKAM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>1024</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>PEP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>1025</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>NTES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43194</th>\n",
       "      <td>43195</td>\n",
       "      <td>2018-04-27</td>\n",
       "      <td>INTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43195</th>\n",
       "      <td>43196</td>\n",
       "      <td>2018-04-27</td>\n",
       "      <td>ADI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43196</th>\n",
       "      <td>43197</td>\n",
       "      <td>2018-04-28</td>\n",
       "      <td>PYPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43197</th>\n",
       "      <td>43198</td>\n",
       "      <td>2018-04-28</td>\n",
       "      <td>EBAY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43198</th>\n",
       "      <td>43199</td>\n",
       "      <td>2018-04-28</td>\n",
       "      <td>NTES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1360 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UID        Date Ticker Sentiment Reason\n",
       "1020    1021  2015-02-09   NTES       NaN    NaN\n",
       "1021    1022  2015-02-09   VRSN       NaN    NaN\n",
       "1022    1023  2015-02-09   AKAM       NaN    NaN\n",
       "1023    1024  2015-02-10    PEP       NaN    NaN\n",
       "1024    1025  2015-02-10   NTES       NaN    NaN\n",
       "...      ...         ...    ...       ...    ...\n",
       "43194  43195  2018-04-27   INTC       NaN    NaN\n",
       "43195  43196  2018-04-27    ADI       NaN    NaN\n",
       "43196  43197  2018-04-28   PYPL       NaN    NaN\n",
       "43197  43198  2018-04-28   EBAY       NaN    NaN\n",
       "43198  43199  2018-04-28   NTES       NaN    NaN\n",
       "\n",
       "[1360 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df_gemini_hb[sentiment_df_gemini_hb['Sentiment'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40280</th>\n",
       "      <td>107038</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40281</th>\n",
       "      <td>107039</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40282</th>\n",
       "      <td>107040</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>INTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40283</th>\n",
       "      <td>107041</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>INTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40284</th>\n",
       "      <td>107042</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>EXC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77206</th>\n",
       "      <td>144114</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>TMUS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77207</th>\n",
       "      <td>144115</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77208</th>\n",
       "      <td>144116</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77209</th>\n",
       "      <td>144117</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>PDD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77210</th>\n",
       "      <td>144118</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>DOCU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1110 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          UID       Date Ticker Sentiment Reason\n",
       "40280  107038 2022-01-04   NVDA       NaN    NaN\n",
       "40281  107039 2022-01-04     ZS       NaN    NaN\n",
       "40282  107040 2022-01-04   INTC       NaN    NaN\n",
       "40283  107041 2022-01-04   INTC       NaN    NaN\n",
       "40284  107042 2022-01-04    EXC       NaN    NaN\n",
       "...       ...        ...    ...       ...    ...\n",
       "77206  144114 2022-12-15   TMUS       NaN    NaN\n",
       "77207  144115 2022-12-15   AMGN       NaN    NaN\n",
       "77208  144116 2022-12-15   AMGN       NaN    NaN\n",
       "77209  144117 2022-12-15    PDD       NaN    NaN\n",
       "77210  144118 2022-12-15   DOCU       NaN    NaN\n",
       "\n",
       "[1110 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df_gemini['Date'] = pd.to_datetime(sentiment_df_gemini['Date'], errors='coerce')\n",
    "sentiment_2022_df = sentiment_df_gemini[sentiment_df_gemini['Date'].dt.year == 2022]\n",
    "sentiment_2022_df[sentiment_2022_df['Sentiment'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38731 entries, 40100 to 78830\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   UID        38731 non-null  int64         \n",
      " 1   Date       38731 non-null  datetime64[ns]\n",
      " 2   Ticker     38731 non-null  object        \n",
      " 3   Sentiment  37621 non-null  object        \n",
      " 4   Reason     37621 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(3)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "sentiment_2022_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78826</th>\n",
       "      <td>145734</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>AEP</td>\n",
       "      <td>Slightly Bearish</td>\n",
       "      <td>American Electric Power closed slightly lower ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78827</th>\n",
       "      <td>145735</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>Tesla's self-driving system requires a human b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78828</th>\n",
       "      <td>145736</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>Tesla's shares surged after Elon Musk reassure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78829</th>\n",
       "      <td>145737</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>BIIB</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Biogen saw high options trading volume.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78830</th>\n",
       "      <td>145738</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Tesla was among the best performing components...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          UID        Date Ticker         Sentiment  \\\n",
       "78826  145734  2022-12-30    AEP  Slightly Bearish   \n",
       "78827  145735  2022-12-30   TSLA           Bearish   \n",
       "78828  145736  2022-12-30   TSLA           Bullish   \n",
       "78829  145737  2022-12-30   BIIB           Neutral   \n",
       "78830  145738  2022-12-30   TSLA           Neutral   \n",
       "\n",
       "                                                  Reason  \n",
       "78826  American Electric Power closed slightly lower ...  \n",
       "78827  Tesla's self-driving system requires a human b...  \n",
       "78828  Tesla's shares surged after Elon Musk reassure...  \n",
       "78829            Biogen saw high options trading volume.  \n",
       "78830  Tesla was among the best performing components...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df_gemini.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
